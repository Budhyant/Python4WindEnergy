{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###PyTables\n",
      "\n",
      "PyTables is a package for managing hierarchical datasets and designed to efficiently and easily cope with extremely large amounts of data. PyTables is built on top of the HDF5 library, using the Python language and the NumPy package. This package is an alternative to [h5py](http://www.h5py.org/) which was discussed before.\n",
      "\n",
      "Features it has to offer:\n",
      "\n",
      "* structure youre data in hierarchical fashion (like in folders/directories) and add user specific data for each group/node\n",
      "\n",
      "* stores, tables or NumPy arrays, or a table of NumPy arrays\n",
      "\n",
      "* optimized for I/O speed\n",
      "\n",
      "* apply out of memory mathermatical operations on the data\n",
      "\n",
      "* file based\n",
      "\n",
      "* mainly ment for reading, be carefull when writing\n",
      "\n",
      "* well integrated within the Python ecosystem (NumPy, pandas, ...)\n",
      "\n",
      "* is not a replacement for a relational SQL database\n",
      "\n",
      "[Home page](http://www.pytables.org/moin)\n",
      "[GitHub dev pages](https://github.com/PyTables/PyTables)\n",
      "[Documentation](http://pytables.github.io/)\n",
      "[Tutorial](http://pytables.github.io/usersguide/tutorials.html)\n",
      "\n",
      "A graphical interface to PyTables data sets exists at [ViTables](http://vitables.org/) (which also supports HDF5 files).\n",
      "\n",
      "Group GPS data by date and store each day as a table or something with Pytables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "import tables as tbl\n",
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Defining the structure of our pytables file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class TableDefs:\n",
      "    class data(tbl.IsDescription):    \n",
      "        Time        = tbl.Int64Col()\n",
      "        Latitude_N  = tbl.Float32Col()\n",
      "        Longitude_E = tbl.Float32Col()\n",
      "        orientation = tbl.Float16Col()\n",
      "        memo        = tbl.StringCol(16)\n",
      "    class statistics(tbl.IsDescription):\n",
      "        memo        = tbl.StringCol(16)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Creating, opening and closing an HDF5 PyTables file"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_db(fpath):\n",
      "    # if the database does not exist, create one\n",
      "    try:\n",
      "        #h5f = open(src).close()\n",
      "        h5f = tbl.openFile(fpath, mode = \"r+\", title = \"gps_db\")\n",
      "        print 'h5file opened:', fpath\n",
      "    except IOError:\n",
      "        # if the file does not exists, create it with the correct columns\n",
      "        # note that the mode w will delete any existing one\n",
      "        h5f = tbl.openFile(fpath, mode = \"w\", title = \"gps_db\")\n",
      "        \n",
      "        # layout global structure\n",
      "        descr = 'GPS Data'\n",
      "        data_group = h5f.createGroup(\"/\", 'data', descr)\n",
      "        \n",
      "        # besides the data as a numpy array, we will also save it as a table\n",
      "        descr = 'GPS data imported from CSV file'\n",
      "        table = h5f.createTable(data_group, 'csv', TableDefs.data, descr)\n",
      "        table.flush()\n",
      "        \n",
      "        # subgroup: data sorted per month\n",
      "        descr = 'Months'\n",
      "        h5f.createGroup(\"/data\", 'months', descr)\n",
      "        \n",
      "        # A table that could hold statistical data per month\n",
      "        descr = 'Monthly Statistics on GPS Data'\n",
      "        table = h5f.createTable(data_group, 'statistics', TableDefs.statistics, descr)\n",
      "        table.flush()\n",
      "        \n",
      "        # If you would close the empty file now the created groups are lost.\n",
      "        \n",
      "        print 'empty h5file created:', fpath\n",
      "    return h5f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to build in some concurrency"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_node(h5f, node, descr=''):\n",
      "    \"\"\"\n",
      "    open in write mode with some concurrency build in\n",
      "    \"\"\"\n",
      "    try:\n",
      "        group_node = h5f.getNode('/'+node)\n",
      "    except tbl.exceptions.NoSuchNodeError:\n",
      "        group_node = h5f.createGroup(\"/\", node, descr)\n",
      "    except Exception as exception:\n",
      "        h5f.close()\n",
      "        raise exception\n",
      "    return group_node"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_table(group, table_name, descr=''):\n",
      "    try:\n",
      "        table = getattr(group, table_name)\n",
      "    except tbl.exceptions.NoSuchNodeError:\n",
      "        table = h5f.createTable(group, table_name, TableDefs.data, descr)\n",
      "        table.flush()\n",
      "    except Exception as exception:\n",
      "        h5f.close()\n",
      "        raise exception\n",
      "    return table"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets save the GPS data as found from the CSV file. Convert datetime object to unix time stamp. There are two ways to do that, and one of them is wrong:\n",
      "```\n",
      "date = '20131211 18:29:30'\n",
      "dt = datetime.strptime(date, \"%Y%m%d %H:%M:%S\")\n",
      "print dt.strftime('%s') # wrong result\n",
      "import calendar\n",
      "print calendar.timegm(dt.utctimetuple())\n",
      "```\n",
      "Use the example from the pandas tutorial with the user defined date parser, but now convert the date to a unix time stamp."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Reads information from csv:\n",
      "def date_parser(date):\n",
      "    # create a datetime instance and assume constant formatting\n",
      "    # format: 20131211 18:29:30\n",
      "    return datetime(int(date[:4]), int(date[4:6]), int(date[6:8]), int(date[9:11]), int(date[12:14]), int(date[15:17]))\n",
      "df = pd.read_csv('gps.csv', header=0, skiprows=[1], parse_dates=[0], date_parser=date_parser)\n",
      "\n",
      "#Creates a HDF5 PyTable \"h5f\" and a file HDF5 named \"gps_db\".\n",
      "h5f = get_db('gps_db.hdf5')\n",
      "data_node = get_node(h5f, 'data')\n",
      "\n",
      "#Creates an array \"Data\" containing the Latitude, Longitude and Orientation in the first, second and third columns respectively.\n",
      "#The array must have dtype = float in order to create a table and write the information into the HDF5 file.\n",
      "data = np.array(df.values[:,1:],dtype='f');\n",
      "\n",
      "#Writes the Data array created previously into the file h5f using the createArray built-in function.\n",
      "h5f.createArray(data_node, 'gpsdata', data, title='gps data');\n",
      "\n",
      "#Closes the file h5f.\n",
      "h5f.close()\n",
      "\n",
      "#np.genfromtxt('gps.csv', converters={0: lambda d: float(s or 0)})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "empty h5file created: gps_db.hdf5\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add all the data in the csv table"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Reads information from csv:\n",
      "df = pd.read_csv('gps.csv', header=0, skiprows=[1], parse_dates=[0], date_parser=date_parser)\n",
      "\n",
      "#Creates a HDF5 PyTable \"h5f\" and a file HDF5 named \"gps_db\".\n",
      "h5f2 = get_db('gps_db2.hdf5')\n",
      "data_node = get_node(h5f2, 'data')\n",
      "\n",
      "#Reads the \"data\" array from the h5f file:\n",
      "#table = h5f.root.data.csv\n",
      "csv_table = get_table(h5f2.root.data, 'csv')\n",
      "\n",
      "#tablerow contains the first row of the csv_table:\n",
      "tablerow = csv_table.row\n",
      "\n",
      "#Reads next rows from panda type database df;\n",
      "for dfrow in df.values:\n",
      "    #tablerow['Time'] = dfrow[0]\n",
      "    tablerow['Latitude_N'] = float(dfrow[1])\n",
      "    tablerow['Longitude_E'] = float(dfrow[2])\n",
      "    tablerow['orientation'] = float(dfrow[3])\n",
      "    tablerow.append()\n",
      "csv_table.flush()\n",
      "\n",
      "#Closes the file h5f.\n",
      "h5f2.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "h5file opened: gps_db2.hdf5\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Example: [store DataFrame in PyTables](http://pandas.pydata.org/pandas-docs/stable/api.html?highlight=pytables#hdfstore-pytables-hdf5)\n",
      "\n",
      "### Excercise:\n",
      "\n",
      "* benchmark: how fast is storing with the hdfstore, compared to native pytables and all the other examples we have seen\n",
      "\n",
      "* Assume you have a huge CSV file that is too big load into memory. Store it safely in a PyTables table."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext logit\n",
      "user_name = 'FcoH'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%logit save_BM_NativePyTableStorage, gps_BM1.hdf5\n",
      "\n",
      "#Storing CSV file values into PyTables. The CSV file is too big to be loaded into memory (RAM), \n",
      "#thus the chosen solution procedure is to read and save the information sequentially. It is \n",
      "#assumed that both the CSV file and the new PyTable can be stored in the local hard disk.\n",
      "\n",
      "#BENCHMARK 1: APPEND ROWS INTO THE PYTABLE (NATIVE FORM).\n",
      "\n",
      "#Creation of an instance object \"h5f\" of a HDF5 file named \"gps_BM1\".\n",
      "#The root of the object tree is specified in the instance\u2019s root attribute.\n",
      "h5f = get_db('gps_BM1.hdf5')\n",
      "\n",
      "#Reads the \"csv\" table from the h5f object, which is inside the \"data\" group in the h5f object.\n",
      "#table = h5f.root.data.csv\n",
      "csv_table = get_table(h5f.root.data, 'csv')\n",
      "\n",
      "#tablerow contains the first row of the csv_table having attributes of type \"data\":\n",
      "tablerow = csv_table.row\n",
      "\n",
      "#Reads information manually from the CSV file and stores it into the gps_BM1.hdf5 file:\n",
      "with open('gps.csv') as f:\n",
      "    f.readline()              #Reads and discards the names of the CSV file.\n",
      "    f.readline()              #Reads and discards the units of the CSV file.\n",
      "    for i in f.readlines():   #Reads the data.\n",
      "        \n",
      "        #Reads the ith row and return a list of strings. Then, parses the list \n",
      "        #into a 32bit float numpy array (i.e., exclude the date/time column).\n",
      "        aux=np.array(i.strip().split(\",\"))[1:].astype(np.float32)\n",
      "        \n",
      "        #Reads the elements of the array and appends them into the csv_table PyTable:\n",
      "        tablerow['Latitude_N'] = aux[0]\n",
      "        tablerow['Longitude_E'] = aux[1]\n",
      "        tablerow['orientation'] = aux[2]\n",
      "        tablerow.append()\n",
      "        \n",
      "        #After the data is processed, we flush the table\u2019s I/O buffer if we want to\n",
      "        #write all this data to disk. We achieve that by calling the table.flush() method:\n",
      "        csv_table.flush()\n",
      "\n",
      "#Closes the h5f file.\n",
      "h5f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "h5file opened: gps_BM1.hdf5\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%logit save_BM_HDFStorage, gps_BM2.hdf5\n",
      "\n",
      "#BENCHMARK 2: APPEND ROWS INTO THE PYTABLE (HDFStore FORM).\n",
      "\n",
      "#Creation of a PyTable \"store\" object and a HDF5 file named \"gps_BM2\".\n",
      "store = pd.HDFStore('gps_BM2.hdf5',mode = \"w\")\n",
      "\n",
      "#store.append('/data',pd.DataFrame(data={'Latitude_N':[], 'Longitude_E':[], 'orientation':[]}))\n",
      "#tablerow=store.root.data.table.row\n",
      "\n",
      "#res = pd.DataFrame(columns=('lib', 'qty1', 'qty2'))                  #Empty DataFrame\n",
      "#row = pd.DataFrame([dict(lib='hello', qty1=4.0, qty2=100.0), ])      #New DataFrame\n",
      "#res = res.append(row, ignore_index=True)                             #Append DataFrame\n",
      "\n",
      "\n",
      "#Reads information manually from the CSV file and stores it into the gps_BM2.hdf5 file:\n",
      "with open('gps.csv') as f:\n",
      "    f.readline()           #Reads and discards the names of the CSV file.\n",
      "    f.readline()           #Reads and discards the units of the CSV file.\n",
      "    indexer=0              #Indexer.\n",
      "    for i in f.readlines():   #Reads the data.\n",
      "        \n",
      "        #Reads the ith row and return a list of strings. Then, parses the list \n",
      "        #into a 32bit float numpy array (i.e., exclude the date/time column).\n",
      "        aux=np.array(i.strip().split(\",\"))[1:].astype(np.float32)\n",
      "        \n",
      "        ##Creates a DataFrame with the information and appends the DataFrame to the data group in the store object:\n",
      "        store.append('/data',pd.DataFrame(data={'Latitude_N':[aux[0]], 'Longitude_E':[aux[1]], 'orientation':[aux[2]]},index=[indexer]))\n",
      "    \n",
      "        #After the data is processed, we flush the table\u2019s I/O buffer if we want to\n",
      "        #write all this data to disk. We achieve that by calling the store.flush() method:\n",
      "        store.flush()\n",
      "        \n",
      "        #Updates indexer:\n",
      "        indexer+=1\n",
      "\n",
      "#Closes the h5f file.\n",
      "store.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%logit save_BM_HDFStorage_Fast, gps_BM3.hdf5\n",
      "\n",
      "#How does the storage process improve when assuming that information can completely be loaded into memory (RAM) before flushing it?\n",
      "#BENCHMARK 3: APPEND ROWS INTO THE PYTABLE (HDFStore FORM with complete loaded info).\n",
      "\n",
      "#Creation of a PyTable \"store\" object and a HDF5 file named \"gps_BM3\".\n",
      "store = pd.HDFStore('gps_BM3.hdf5',mode = \"w\")\n",
      "\n",
      "#Function for date parsing:\n",
      "def date_parser(date):\n",
      "    # create a datetime instance and assume constant formatting\n",
      "    # format: 20131211 18:29:30\n",
      "    return datetime(int(date[:4]), int(date[4:6]), int(date[6:8]), int(date[9:11]), int(date[12:14]), int(date[15:17]))\n",
      "\n",
      "#Reads information from csv:\n",
      "df = pd.read_csv('gps.csv', header=0, skiprows=[1], parse_dates=[0], date_parser=date_parser)\n",
      "\n",
      "#Appends information to the data group in the store object:\n",
      "store.append('/data',df)\n",
      "\n",
      "#After the data is processed, we flush the table\u2019s I/O buffer if we want to\n",
      "#write all this data to disk. We achieve that by calling the store.flush() method:\n",
      "store.flush()\n",
      "\n",
      "#Closes the h5f file.\n",
      "store.close()\n",
      "\n",
      "#The file will have a larger size since the dates are included."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "save_BM_HDFStorage_Fast :\n",
        "14-02-24 20:41:02\n",
        "File size: 5.62045192719 MB\n",
        "Creation time: 1.02662579961 sec\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Any other method that does not involve the creation of tables (such as the ones studied in the Simple data storage IPython notebook)\n",
      "#will perform better."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}